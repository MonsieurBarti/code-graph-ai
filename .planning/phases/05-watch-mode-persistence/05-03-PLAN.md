---
phase: 05-watch-mode-persistence
plan: 03
type: execute
wave: 3
depends_on: ["05-01", "05-02"]
files_modified:
  - src/mcp/server.rs
  - src/mcp/mod.rs
  - src/cli.rs
  - src/main.rs
autonomous: true
requirements: [INTG-04, INTG-05, PERF-04]

must_haves:
  truths:
    - "MCP server lazily starts watcher on first graph access and keeps it running"
    - "Watcher events trigger incremental re-index that updates the cached graph in the MCP server"
    - "Cold start loads cached graph from .code-graph/graph.bin and applies staleness diff"
    - "code-graph watch <path> CLI command starts standalone watcher with terminal status output"
    - "Graph is saved to cache after full build and after incremental updates"
    - "Graph cache uses RwLock for concurrent read access during MCP tool calls"
    - "Watcher event loop never holds RwLock write guard during parse/resolve/IO — clones graph, drops lock, does work, re-locks only to swap"
  artifacts:
    - path: "src/mcp/server.rs"
      provides: "RwLock upgrade, lazy watcher start, event processing loop, cache-aware graph resolution"
      contains: "RwLock"
    - path: "src/mcp/mod.rs"
      provides: "Updated run() function"
      contains: "run"
    - path: "src/cli.rs"
      provides: "Watch CLI subcommand"
      contains: "Watch"
    - path: "src/main.rs"
      provides: "Watch command handler with standalone watcher loop"
      contains: "Commands::Watch"
  key_links:
    - from: "src/mcp/server.rs"
      to: "src/watcher/mod.rs"
      via: "start_watcher called lazily in resolve_graph, events processed in background task"
      pattern: "start_watcher"
    - from: "src/mcp/server.rs"
      to: "src/cache/envelope.rs"
      via: "load_cache on cold start, save_cache after build/update"
      pattern: "load_cache.*save_cache"
    - from: "src/mcp/server.rs"
      to: "src/watcher/incremental.rs"
      via: "handle_file_event called inside spawn_blocking with NO RwLock held; write lock acquired only to swap result"
      pattern: "handle_file_event"
    - from: "src/main.rs"
      to: "src/watcher/mod.rs"
      via: "Commands::Watch starts standalone watcher with terminal output"
      pattern: "start_watcher"
---

<objective>
Integrate the watcher and cache into the MCP server, add cold start with staleness diff, and provide the standalone Watch CLI command.

Purpose: This ties everything together — the MCP server now loads cached graphs on cold start (PERF-04), lazily starts the watcher on first use (INTG-04), processes incremental updates in the background (INTG-05), and provides a standalone `code-graph watch` command for debugging. The graph cache is upgraded from Mutex to RwLock for concurrent read access.

Output: Fully integrated MCP server with watcher + cache, Watch CLI command, cold start staleness diff.
</objective>

<execution_context>
@/Users/monsieurbarti/.claude/get-shit-done/workflows/execute-plan.md
@/Users/monsieurbarti/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-watch-mode-persistence/05-RESEARCH.md
@.planning/phases/05-watch-mode-persistence/05-01-SUMMARY.md
@.planning/phases/05-watch-mode-persistence/05-02-SUMMARY.md
@.planning/phases/04-mcp-integration/04-02-SUMMARY.md
@src/mcp/server.rs
@src/mcp/mod.rs
@src/cli.rs
@src/main.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: MCP server integration — RwLock, cold start, lazy watcher, event loop</name>
  <files>
    src/mcp/server.rs
    src/mcp/mod.rs
  </files>
  <action>
    Major refactor of `src/mcp/server.rs` to integrate watcher + cache:

    **1. Replace Mutex with RwLock on graph_cache:**
    ```rust
    use tokio::sync::RwLock;
    // Change graph_cache field:
    graph_cache: Arc<RwLock<HashMap<PathBuf, Arc<CodeGraph>>>>,
    ```
    Update `new()` accordingly.

    **2. Add watcher handle field:**
    ```rust
    watcher_handle: Arc<tokio::sync::Mutex<Option<crate::watcher::WatcherHandle>>>,
    ```
    This is a tokio Mutex (not RwLock) since it's only written once (lazy init) and rarely read.

    **3. Rewrite resolve_graph to support cold start + cache + lazy watcher:**

    The new `resolve_graph` flow:
    a. Determine effective path (override > default)
    b. Try read lock on cache — if graph exists, return it
    c. Drop read lock, acquire write lock
    d. Double-check cache (another task may have populated between locks)
    e. Try loading from disk cache via `load_cache()`
    f. If cache hit: apply staleness diff (compare file mtimes, re-parse changed files)
    g. If cache miss: full build via `build_graph()`
    h. Save graph to disk cache via `save_cache()`
    i. Insert into in-memory cache
    j. Start watcher lazily if not already running
    k. Return graph

    ```rust
    async fn resolve_graph(
        &self,
        project_path_override: Option<&str>,
    ) -> Result<(Arc<CodeGraph>, PathBuf), String> {
        let path: PathBuf = match project_path_override {
            Some(p) => PathBuf::from(p),
            None => (*self.default_project_root).clone(),
        };

        // Fast path: read lock
        {
            let cache = self.graph_cache.read().await;
            if let Some(graph) = cache.get(&path) {
                return Ok((Arc::clone(graph), path));
            }
        }

        // Slow path: write lock — build or load
        let mut cache = self.graph_cache.write().await;
        // Double-check after acquiring write lock
        if let Some(graph) = cache.get(&path) {
            return Ok((Arc::clone(graph), path));
        }

        // Try disk cache first (cold start)
        let path_clone = path.clone();
        let graph = tokio::task::spawn_blocking(move || -> Result<CodeGraph, String> {
            if let Some(envelope) = crate::cache::load_cache(&path_clone) {
                // Apply staleness diff
                let graph = apply_staleness_diff(envelope, &path_clone)
                    .map_err(|e| e.to_string())?;
                // Save updated cache
                let _ = crate::cache::save_cache(&path_clone, &graph);
                Ok(graph)
            } else {
                // Full build
                let graph = crate::build_graph(&path_clone, false)
                    .map_err(|e| e.to_string())?;
                // Save to cache
                let _ = crate::cache::save_cache(&path_clone, &graph);
                Ok(graph)
            }
        })
        .await
        .map_err(|e| format!("task join error: {}", e))?
        .map_err(|e| e)?;

        if graph.file_index.is_empty() {
            return Err(format!(
                "No indexed files found at '{}'. Run 'code-graph index <path>' first.",
                path.display()
            ));
        }

        let graph = Arc::new(graph);
        cache.insert(path.clone(), Arc::clone(&graph));

        // Start watcher lazily
        self.ensure_watcher_running(&path).await;

        Ok((graph, path))
    }
    ```

    **4. Add the staleness diff function (in server.rs or as a helper):**

    ```rust
    /// Apply staleness diff: compare cached file mtimes against current filesystem,
    /// re-parse changed/new files, remove deleted files.
    fn apply_staleness_diff(
        envelope: crate::cache::CacheEnvelope,
        project_root: &Path,
    ) -> anyhow::Result<CodeGraph> {
        let mut graph = envelope.graph;
        let cached_mtimes = envelope.file_mtimes;

        // Walk current files
        let config = crate::config::CodeGraphConfig::load(project_root);
        let current_files = crate::walker::walk_project(project_root, &config, false)?;
        let current_set: std::collections::HashSet<PathBuf> = current_files.iter().cloned().collect();

        // Find changed and new files
        let mut files_to_reparse: Vec<PathBuf> = Vec::new();
        for file in &current_files {
            if let Ok(metadata) = std::fs::metadata(file) {
                let mtime_secs = metadata
                    .modified()
                    .ok()
                    .and_then(|t| t.duration_since(std::time::UNIX_EPOCH).ok())
                    .map(|d| d.as_secs())
                    .unwrap_or(0);
                let size = metadata.len();

                match cached_mtimes.get(file) {
                    Some(cached) if cached.mtime_secs == mtime_secs && cached.size == size => {
                        // Unchanged — skip
                    }
                    _ => {
                        // Changed or new — needs re-parse
                        files_to_reparse.push(file.clone());
                    }
                }
            }
        }

        // Find deleted files (in cache but not on disk)
        let deleted_files: Vec<PathBuf> = cached_mtimes
            .keys()
            .filter(|p| !current_set.contains(*p))
            .cloned()
            .collect();

        // Remove deleted files from graph
        for path in &deleted_files {
            graph.remove_file_from_graph(path);
        }

        // Re-parse changed/new files (remove old, add new)
        for path in &files_to_reparse {
            graph.remove_file_from_graph(path);

            let source = match std::fs::read(path) {
                Ok(s) => s,
                Err(_) => continue,
            };

            let language_str = match path.extension().and_then(|e| e.to_str()).unwrap_or("") {
                "ts" => "typescript",
                "tsx" => "tsx",
                "js" | "jsx" => "javascript",
                _ => continue,
            };

            let result = match crate::parser::parse_file(path, &source) {
                Ok(r) => r,
                Err(_) => continue,
            };

            let file_idx = graph.add_file(path.clone(), language_str);
            for (symbol, children) in &result.symbols {
                let sym_idx = graph.add_symbol(file_idx, symbol.clone());
                for child in children {
                    graph.add_child_symbol(sym_idx, child.clone());
                }
            }
        }

        // If any files were re-parsed, run full resolve to fix import edges.
        // This is simpler than scoped resolve for cold-start diff (only happens once).
        if !files_to_reparse.is_empty() || !deleted_files.is_empty() {
            // Collect parse results for re-parsed files
            let mut parse_results = std::collections::HashMap::new();
            for path in &files_to_reparse {
                if let Ok(source) = std::fs::read(path) {
                    if let Ok(result) = crate::parser::parse_file(path, &source) {
                        parse_results.insert(path.clone(), result);
                    }
                }
            }
            // Re-resolve only the changed files' imports
            // For simplicity on cold start, do a full resolve pass if there were changes
            // The full resolve pass is safe — it just adds edges, and we already removed stale ones
            if !parse_results.is_empty() {
                // Build parse_results for ALL files in graph for full resolve
                let mut all_parse_results = std::collections::HashMap::new();
                for file_path in graph.file_index.keys() {
                    if let Ok(source) = std::fs::read(file_path) {
                        if let Ok(result) = crate::parser::parse_file(file_path, &source) {
                            all_parse_results.insert(file_path.clone(), result);
                        }
                    }
                }
                crate::resolver::resolve_all(&mut graph, project_root, &all_parse_results, false);
            }
        }

        Ok(graph)
    }
    ```

    **IMPORTANT:** The staleness diff resolve approach above is correct but not optimized for large codebases with many changes. For cold start this is acceptable since it only runs once. The incremental watcher (Plan 02) uses scoped resolve for single-file changes.

    **UPDATE:** On reflection, re-parsing ALL files just for resolve is expensive on cold start if only a few files changed. Better approach: if < 10% of files changed, do scoped re-resolve (re-parse only changed files, resolve just their imports). If >= 10% changed, do full rebuild. Implement the simpler threshold approach:

    If `files_to_reparse.len() + deleted_files.len() > current_files.len() / 10`:
      - Discard cached graph, do full rebuild via `build_graph()`
    Else:
      - Use scoped approach above (remove + re-add changed files, skip full resolve for unchanged)

    **5. Add ensure_watcher_running method:**

    **CRITICAL: Lock discipline (research Pitfall 2 avoidance).**
    The event processing loop MUST NOT hold the RwLock write guard during CPU-bound
    work (parse_file, resolve_import) or blocking I/O (save_cache). Holding a write
    lock during these operations blocks ALL concurrent MCP tool calls (which need read
    access) for the full re-parse duration (50-100ms+), defeating the RwLock upgrade.

    The correct pattern is:
    1. Acquire **read** lock, clone the Arc<CodeGraph>, drop read lock immediately
    2. Clone the graph data from the Arc (no lock held)
    3. Run handle_file_event + save_cache on the cloned graph (no lock held — CPU/IO happens here)
    4. Acquire **write** lock only to swap in Arc::new(updated_graph), drop write lock immediately

    This ensures the write lock is held only for the HashMap insert (nanoseconds),
    not during parse/resolve (milliseconds).

    ```rust
    async fn ensure_watcher_running(&self, project_root: &Path) {
        let mut watcher_guard = self.watcher_handle.lock().await;
        if watcher_guard.is_some() {
            return; // already running
        }

        match crate::watcher::start_watcher(project_root) {
            Ok((handle, mut rx)) => {
                // Spawn background task to process events
                let graph_cache = Arc::clone(&self.graph_cache);
                let root = project_root.to_path_buf();
                tokio::spawn(async move {
                    while let Some(event) = rx.recv().await {
                        let root = root.clone();
                        let graph_cache = Arc::clone(&graph_cache);

                        match event {
                            crate::watcher::event::WatchEvent::ConfigChanged => {
                                // Full rebuild — all CPU work happens in spawn_blocking
                                // with NO lock held. Write lock acquired only to swap result.
                                let root_clone = root.clone();
                                if let Ok(new_graph) = tokio::task::spawn_blocking(move || {
                                    let graph = crate::build_graph(&root_clone, false)?;
                                    let _ = crate::cache::save_cache(&root_clone, &graph);
                                    Ok::<_, anyhow::Error>(graph)
                                }).await.ok().and_then(|r| r.ok()) {
                                    // Write lock held ONLY for the insert (nanoseconds)
                                    let mut cache = graph_cache.write().await;
                                    cache.insert(root.clone(), Arc::new(new_graph));
                                    // Lock dropped immediately
                                }
                            }
                            _ => {
                                // Incremental update — clone graph WITHOUT holding write lock.

                                // Step 1: Read lock to clone the Arc (fast)
                                let old_arc = {
                                    let cache = graph_cache.read().await;
                                    cache.get(&root).cloned()
                                    // Read lock dropped here
                                };

                                if let Some(old_arc) = old_arc {
                                    // Step 2: Clone graph data from Arc (no lock held)
                                    let mut graph = (*old_arc).clone();

                                    // Step 3: CPU-bound parse/resolve + blocking IO
                                    //         ALL happen with NO lock held
                                    let root_for_blocking = root.clone();
                                    let result = tokio::task::spawn_blocking(move || {
                                        let modified = crate::watcher::incremental::handle_file_event(
                                            &mut graph, &event, &root_for_blocking,
                                        );
                                        if modified {
                                            let _ = crate::cache::save_cache(&root_for_blocking, &graph);
                                        }
                                        (graph, modified)
                                    }).await;

                                    // Step 4: Write lock ONLY to swap in result (nanoseconds)
                                    if let Ok((graph, true)) = result {
                                        let mut cache = graph_cache.write().await;
                                        cache.insert(root.clone(), Arc::new(graph));
                                        // Lock dropped immediately
                                    }
                                }
                            }
                        }
                    }
                });
                *watcher_guard = Some(handle);
            }
            Err(e) => {
                eprintln!("[watcher] failed to start: {}", e);
            }
        }
    }
    ```

    **6. Update all tool handlers for RwLock:**
    The tool handlers call `self.resolve_graph()` which handles locking internally. No changes needed to tool handler code since resolve_graph returns `Arc<CodeGraph>` — the handlers never hold the lock directly.

    **src/mcp/mod.rs:**
    No changes needed — `run()` still creates CodeGraphServer and calls serve_server.

    **Critical anti-pattern avoidance (enforced in code above):**
    - NEVER hold the RwLock write guard during parse/resolve/IO — clone graph, drop lock, do work, re-lock only to swap (per research Pitfall 2)
    - The ConfigChanged branch runs build_graph + save_cache inside spawn_blocking with NO lock; write lock is acquired only for the final insert
    - The incremental branch acquires READ lock to clone the Arc, drops it, does all work in spawn_blocking with NO lock, then acquires WRITE lock only for the final insert
    - Both branches ensure write lock duration is nanoseconds (HashMap insert), not milliseconds (parse/resolve)
  </action>
  <verify>
    <automated>cd /workspace && cargo build 2>&1 | tail -5</automated>
    <manual>Verify MCP server compiles with RwLock, watcher integration, and cache loading</manual>
  </verify>
  <done>MCP server uses RwLock for graph cache. resolve_graph tries disk cache first (cold start), applies staleness diff, falls back to full build. Watcher starts lazily on first graph access. Background task processes watcher events with incremental updates. Graph saved to cache after builds and updates. Event loop NEVER holds write lock during parse/resolve/IO — clones graph under read lock, drops lock, does all CPU/IO work in spawn_blocking with no lock, re-acquires write lock only for the final Arc swap (nanoseconds, not milliseconds).</done>
</task>

<task type="auto">
  <name>Task 2: Watch CLI command and cache save in index command</name>
  <files>
    src/cli.rs
    src/main.rs
  </files>
  <action>
    **src/cli.rs:**
    Add `Watch` variant to the `Commands` enum:
    ```rust
    /// Start a file watcher that monitors for changes and re-indexes incrementally.
    ///
    /// Useful for debugging watcher behavior. The MCP server starts its own
    /// embedded watcher automatically — this command runs standalone.
    Watch {
        /// Path to the project root to watch.
        path: PathBuf,
    },
    ```

    **src/main.rs:**
    Add handler for `Commands::Watch` in the main match block:

    ```rust
    Commands::Watch { path } => {
        eprintln!("Indexing {}...", path.display());
        let mut graph = build_graph(&path, false)?;
        eprintln!(
            "Indexed {} files, {} symbols. Starting watcher...",
            graph.file_count(),
            graph.symbol_count()
        );

        // Save initial cache
        if let Err(e) = cache::save_cache(&path, &graph) {
            eprintln!("[cache] failed to save: {}", e);
        }

        // Start watcher
        let (handle, mut rx) = watcher::start_watcher(&path)
            .map_err(|e| anyhow::anyhow!("failed to start watcher: {}", e))?;

        // Keep handle alive
        let _handle = handle;

        eprintln!("Watching for changes... (press Ctrl+C to stop)");

        // Process events
        while let Some(event) = rx.recv().await {
            match &event {
                watcher::event::WatchEvent::Modified(p) => {
                    let start = std::time::Instant::now();
                    watcher::incremental::handle_file_event(&mut graph, &event, &path);
                    let elapsed = start.elapsed();
                    eprintln!(
                        "[watch] modified: {} ({:.1}ms, {} files, {} symbols)",
                        p.strip_prefix(&path).unwrap_or(p).display(),
                        elapsed.as_secs_f64() * 1000.0,
                        graph.file_count(),
                        graph.symbol_count()
                    );
                    let _ = cache::save_cache(&path, &graph);
                }
                watcher::event::WatchEvent::Created(p) => {
                    let start = std::time::Instant::now();
                    watcher::incremental::handle_file_event(&mut graph, &event, &path);
                    let elapsed = start.elapsed();
                    eprintln!(
                        "[watch] created: {} ({:.1}ms, {} files, {} symbols)",
                        p.strip_prefix(&path).unwrap_or(p).display(),
                        elapsed.as_secs_f64() * 1000.0,
                        graph.file_count(),
                        graph.symbol_count()
                    );
                    let _ = cache::save_cache(&path, &graph);
                }
                watcher::event::WatchEvent::Deleted(p) => {
                    watcher::incremental::handle_file_event(&mut graph, &event, &path);
                    eprintln!(
                        "[watch] deleted: {} ({} files, {} symbols)",
                        p.strip_prefix(&path).unwrap_or(p).display(),
                        graph.file_count(),
                        graph.symbol_count()
                    );
                    let _ = cache::save_cache(&path, &graph);
                }
                watcher::event::WatchEvent::ConfigChanged => {
                    eprintln!("[watch] config changed — full re-index...");
                    let start = std::time::Instant::now();
                    graph = build_graph(&path, false)?;
                    let elapsed = start.elapsed();
                    eprintln!(
                        "[watch] re-indexed in {:.1}ms ({} files, {} symbols)",
                        elapsed.as_secs_f64() * 1000.0,
                        graph.file_count(),
                        graph.symbol_count()
                    );
                    let _ = cache::save_cache(&path, &graph);
                }
            }
        }
    }
    ```

    Also update the `Commands::Index` handler to save cache after indexing:
    After the `print_summary(&stats, json);` line, add:
    ```rust
    // Save graph to disk cache for fast cold starts
    if let Err(e) = cache::save_cache(&path, &graph) {
        if verbose {
            eprintln!("  Cache save failed: {}", e);
        }
    }
    ```

    Note: The standalone Watch command output goes to stderr (per established convention from Phase 1). This is user-visible terminal output for debugging, not structured data. The MCP embedded watcher is silent (no terminal output — it communicates through the MCP protocol only).
  </action>
  <verify>
    <automated>cd /workspace && cargo build 2>&1 | tail -5 && ./target/debug/code-graph --help 2>&1 | grep -i watch</automated>
    <manual>Verify `code-graph watch` appears in help output and binary compiles</manual>
  </verify>
  <done>Watch CLI subcommand added — starts standalone watcher with terminal status output showing file events and timing. Index command now saves cache after indexing. All watcher event types handled with timing info. Binary compiles and `watch` appears in help.</done>
</task>

</tasks>

<verification>
1. `cargo build` succeeds with zero new errors
2. `cargo test` passes all existing tests (87+)
3. `code-graph --help` shows `watch` subcommand
4. MCP server resolve_graph tries disk cache first, falls back to full build
5. Watcher starts lazily on first MCP tool call
6. Background event loop processes incremental updates
7. RwLock allows concurrent reads during tool calls
8. Index command saves cache to .code-graph/graph.bin
9. Watch command prints terminal status on file changes
</verification>

<success_criteria>
- MCP server graph_cache uses RwLock (not Mutex)
- resolve_graph loads from disk cache on cold start with staleness diff
- Watcher starts lazily on first graph access (not server startup)
- Background task processes watcher events: incremental for file changes, full rebuild for config changes
- Event loop never holds write lock during parse/resolve/IO (clone → drop lock → work → re-lock to swap)
- Graph saved to disk after build and after incremental updates
- Watch CLI command exists: `code-graph watch <path>`
- Watch command prints timing info per event to stderr
- Index command saves cache after indexing
- All tests pass, binary compiles
</success_criteria>

<output>
After completion, create `.planning/phases/05-watch-mode-persistence/05-03-SUMMARY.md`
</output>
